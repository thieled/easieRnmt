% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/01_preprocessing.R
\name{clean_text}
\alias{clean_text}
\title{Clean, Normalize, and Tokenize Text}
\usage{
clean_text(
  x,
  text_col = "text",
  id_col = NULL,
  lang_guess_col = NULL,
  replace_emojis = TRUE,
  replace_alphaless = TRUE,
  max_char = 5000,
  tokenize_sentences = TRUE,
  max_words = 30,
  clean_characters = TRUE,
  return_string = FALSE,
  verbose = TRUE
)
}
\arguments{
\item{x}{Character vector or data.frame containing texts to clean.}

\item{text_col}{Character, name of the text column if \code{x} is a data.frame.
Ignored if \code{x} is a character vector. Default = "text".}

\item{id_col}{Character, optional column name in the input data.frame to
preserve as \code{id}. Default = NULL.}

\item{lang_guess_col}{Character, optional column name in the input data.frame
to preserve as \code{lang_guess}. Default = NULL.}

\item{replace_emojis}{Logical, whether to replace emojis with placeholder
names. Default = TRUE.}

\item{replace_alphaless}{Logical, whether to replace strings that contain no
alphabetic characters with empty strings. Default = TRUE.}

\item{max_char}{Integer, maximum number of characters per text. Texts longer
than this are truncated. Default = 5000.}

\item{tokenize_sentences}{Logical, whether to split texts into sentences and
chunks. Default = TRUE.}

\item{max_words}{Integer, maximum number of words per sentence or chunk.
Long sentences are split into chunks of this size. Default = 30.}

\item{clean_characters}{Logical, whether to apply character normalization
(regex replacements, squishing, punctuation handling). Default = TRUE.}

\item{return_string}{Logical, if TRUE, return only the cleaned character
vector (\code{text_clean}) instead of a full data.table. Default = FALSE.}

\item{verbose}{Logical, whether to print progress messages. Default = TRUE.}
}
\value{
Either:
\itemize{
\item A data.table with columns:
\itemize{
\item \code{sen_id}: Unique identifier for each sentence or chunk.
\item \code{doc_idx}: Row index of the original document in the input.
\item \code{sen_idx}: Sentence/chunk index within each document.
\item \code{id}: Optional user-provided identifier.
\item \code{text_orig}: Original text.
\item \code{text_clean}: Cleaned and normalized text.
\item \code{lang_guess}: Optional language guess column, if present.
}
\item Or a character vector of cleaned texts if \code{return_string = TRUE}.
}
}
\description{
Cleans and optionally tokenizes raw text data. Handles emoji replacement,
removal of unsupported characters, normalization, sentence tokenization, and
chunking into word-limited segments. Returns either a standardized data.table
or a character vector.
}
